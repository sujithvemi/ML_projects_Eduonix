{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLTK and text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word and Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text.', 'We are learning nltk, text-classification using Python.', 'Very excited!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"This is a sample text. We are learning nltk, text-classification using Python. Very excited!\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'text', '.', 'We', 'are', 'learning', 'nltk', ',', 'text-classification', 'using', 'Python', '.', 'Very', 'excited', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some sample text, showing off the stop words filtration.\n",
      "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
    "stop_set = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sent = [word for word in word_tokens if word not in stop_set]\n",
    "print(example_sent)\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', u'rider', 'are', u'ride', 'their', u'hors', ',', 'they', 'often', 'think', 'of', 'how', u'cowboy', 'rode', u'hors', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "sample_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
    "sample_words = word_tokenize(sample_text)\n",
    "stem_words = [ps.stem(w) for w in sample_words]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_sample = state_union.raw('2005-GWBush.txt')\n",
    "test_sample = state_union.raw('2006-GWBush.txt')\n",
    "custom_tokenizer = PunktSentenceTokenizer(train_sample)\n",
    "tokenized_test = custom_tokenizer.tokenize(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [nltk.pos_tag(word_tokenize(sent)) for sent in tokenized_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'PRESIDENT', 'NNP'), (u'GEORGE', 'NNP'), (u'W.', 'NNP'), (u'BUSH', 'NNP'), (u\"'S\", 'POS'), (u'ADDRESS', 'NNP'), (u'BEFORE', 'IN'), (u'A', 'NNP'), (u'JOINT', 'NNP'), (u'SESSION', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'CONGRESS', 'NNP'), (u'ON', 'NNP'), (u'THE', 'NNP'), (u'STATE', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'UNION', 'NNP'), (u'January', 'NNP'), (u'31', 'CD'), (u',', ','), (u'2006', 'CD'), (u'THE', 'NNP'), (u'PRESIDENT', 'NNP'), (u':', ':'), (u'Thank', 'NNP'), (u'you', 'PRP'), (u'all', 'DT'), (u'.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "docs = [(list(movie_reviews.words(fileid)), category)\n",
    "       for category in movie_reviews.categories()\n",
    "       for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(docs)\n",
    "vocab = [w.lower() for w in movie_reviews.words()]\n",
    "vocab_dist = nltk.FreqDist(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "([u'in', u'the', u'series', u'of', u'the', u'erotic', u'thrillers', u'that', u'flooded', u'the', u'videoshelves', u'in', u'the', u'early', u'1990s', u'came', u'this', u'french', u'-', u'canadian', u'co', u'-', u'production', u'by', u'max', u'fischer', u'.', u'the', u'movie', u'is', u'set', u'in', u'paris', u'where', u'its', u'hero', u',', u'struggling', u'american', u'author', u'david', u'mirkine', u'(', u'judd', u'nelson', u',', u'at', u'the', u'time', u'specialised', u'in', u'playing', u'losers', u'and', u'people', u'at', u'the', u'edge', u'of', u'sanity', u')', u'suffers', u'a', u'terrible', u'writers', u'bloc', u'.', u'he', u'manages', u'to', u'overcome', u'crisis', u'after', u'beginning', u'romantic', u'relationship', u'with', u'beautiful', u'model', u'anabelle', u'(', u'laurence', u'treill', u')', u'.', u'unfortunately', u',', u'she', u'hangs', u'out', u'in', u'jet', u'set', u'circles', u',', u'which', u'gradually', u'makes', u'mirkine', u'pathologically', u'jealous', u'.', u'her', u'connection', u'with', u'powerful', u'david', u'caravan', u'(', u'pierce', u'brosnan', u')', u'would', u'make', u'mirkine', u'step', u'the', u'line', u'between', u'reason', u'and', u'sanity', u'and', u'put', u'in', u'motion', u'whole', u'series', u'of', u'violent', u'and', u'tragic', u'events', u'.', u'although', u'few', u'pseudoerotic', u'scenes', u'and', u'some', u'elements', u'of', u'the', u'plot', u'do', u'indeed', u'make', u'this', u'film', u'an', u'erotic', u'thriller', u',', u'drama', u'would', u'probably', u'be', u'more', u'appropriate', u'genre', u'label', u'.', u'pacing', u'of', u'the', u'film', u'is', u'simply', u'too', u'slow', u'to', u'thrill', u'the', u'viewers', u',', u'and', u'those', u'patient', u'enough', u'to', u'sit', u'through', u'its', u'entirety', u'have', u'to', u'wait', u'a', u'long', u'time', u'between', u'any', u'interesting', u'or', u'significant', u'developments', u'.', u'low', u'budget', u'also', u'becomes', u'painfully', u'visible', u',', u'and', u'physical', u'attributes', u'of', u'laurence', u'treill', u'are', u'the', u'only', u'thing', u'worth', u'watching', u'in', u'this', u'film', u'.', u'unfortunately', u',', u'this', u'film', u'has', u'too', u'little', u'of', u'those', u'supposedly', u'erotic', u'scenes', u',', u'so', u'those', u'who', u'wanted', u'to', u'watch', u'erotic', u'thriller', u'have', u'all', u'the', u'reasons', u'to', u'feel', u'disappointed', u'.', u'instead', u',', u'they', u'would', u'have', u'to', u'settle', u'with', u'pointless', u',', u'slow', u'scenes', u'involving', u'nelson', u',', u'brosnan', u'and', u'laurence', u'treill', u\"'\", u's', u'attempts', u'of', u'serious', u'acting', u'.', u'all', u'of', u'those', u'definitely', u'not', u'worth', u'spending', u'hour', u'and', u'half', u'of', u'someone', u\"'\", u's', u'precious', u'time', u'.'], u'neg')\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "word_features = list(vocab_dist.keys())[:4000]\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [({w:(w in rev) for w in word_features},category) for (rev, category) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 1\n",
    "train_data, test_data = train_test_split(featuresets, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.678\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel='linear'))\n",
    "model.train(train_data)\n",
    "accuracy = nltk.classify.accuracy(model, test_data)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
